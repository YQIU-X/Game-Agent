#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
PPOè®­ç»ƒè„šæœ¬ - æ”¯æŒå®žæ—¶æ—¥å¿—è¾“å‡ºå’ŒæŒ‡æ ‡è®°å½•
"""

import sys
import os
import json
import time
import argparse
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
sys.path.insert(0, project_root)

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.distributions import Categorical
import gym_super_mario_bros
from nes_py.wrappers import JoypadSpace
from gym_super_mario_bros.actions import SIMPLE_MOVEMENT, COMPLEX_MOVEMENT

# å¯¼å…¥é¡¹ç›®æ¨¡å—
from python.models.cnnppo import CNNPPO
from python.wrappers.mario_wrappers import wrap_environment

class PPOTrainer:
    def __init__(self, config):
        self.config = config
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # è®­ç»ƒå‚æ•°
        self.learning_rate = config.get('learning_rate', 0.0003)
        self.gamma = config.get('gamma', 0.99)
        self.clip_ratio = config.get('clip_ratio', 0.2)
        self.value_loss_coef = config.get('value_loss_coef', 0.5)
        self.entropy_coef = config.get('entropy_coef', 0.01)
        self.max_grad_norm = config.get('max_grad_norm', 0.5)
        self.ppo_epochs = config.get('ppo_epochs', 4)
        
        # è®­ç»ƒçŠ¶æ€
        self.episode = 0
        self.total_steps = 0
        self.best_reward = -float('inf')
        
        # æŒ‡æ ‡è®°å½•
        self.episode_rewards = []
        self.value_losses = []
        self.policy_losses = []
        self.total_losses = []
        
        # åˆå§‹åŒ–çŽ¯å¢ƒå’Œæ¨¡åž‹
        self._setup_environment()
        self._setup_model()
        
    def _setup_environment(self):
        """è®¾ç½®è®­ç»ƒçŽ¯å¢ƒ"""
        env_name = self.config.get('env_name', 'SuperMarioBros-1-1-v0')
        action_space = self.config.get('action_space', 'SIMPLE')
        
        # åˆ›å»ºçŽ¯å¢ƒ
        self.env = gym_super_mario_bros.make(env_name)
        
        # è®¾ç½®åŠ¨ä½œç©ºé—´
        if action_space == 'SIMPLE':
            self.env = JoypadSpace(self.env, SIMPLE_MOVEMENT)
            self.action_list = SIMPLE_MOVEMENT
        else:
            self.env = JoypadSpace(self.env, COMPLEX_MOVEMENT)
            self.action_list = COMPLEX_MOVEMENT
            
        # åº”ç”¨åŒ…è£…å™¨
        self.env = wrap_environment(self.env, model_type='CNNPPO')
        
        # èŽ·å–çŽ¯å¢ƒä¿¡æ¯
        obs = self.env.reset()
        if isinstance(obs, tuple):
            obs = obs[0]
        
        self.obs_shape = obs.shape
        self.num_actions = len(self.action_list)
        
        self.log(f"çŽ¯å¢ƒè®¾ç½®å®Œæˆ: {env_name}, åŠ¨ä½œç©ºé—´: {action_space}, è§‚å¯Ÿå½¢çŠ¶: {self.obs_shape}, åŠ¨ä½œæ•°é‡: {self.num_actions}")
        
    def _setup_model(self):
        """è®¾ç½®PPOæ¨¡åž‹"""
        # åˆ›å»ºæ¨¡åž‹
        self.model = CNNPPO(self.obs_shape, self.num_actions).to(self.device)
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)
        
        self.log(f"æ¨¡åž‹è®¾ç½®å®Œæˆ: CNNPPO, è®¾å¤‡: {self.device}")
        
    def log(self, message):
        """è¾“å‡ºæ—¥å¿—"""
        timestamp = datetime.now().strftime("%H:%M:%S")
        log_message = f"[{timestamp}] {message}"
        print(log_message, file=sys.stderr)
        
        # å‘é€åˆ°stdoutä¾›å‰ç«¯èŽ·å–
        try:
            print(json.dumps({
                'timestamp': timestamp,
                'message': message,
                'type': 'log'
            }), flush=True)
        except:
            pass
            
    def log_metrics(self, metrics):
        """è¾“å‡ºè®­ç»ƒæŒ‡æ ‡"""
        try:
            print(json.dumps({
                'timestamp': datetime.now().strftime("%H:%M:%S"),
                'metrics': metrics,
                'type': 'metrics'
            }), flush=True)
        except:
            pass
            
    def collect_episode(self):
        """æ”¶é›†ä¸€ä¸ªepisodeçš„æ•°æ®"""
        obs = self.env.reset()
        if isinstance(obs, tuple):
            obs = obs[0]
            
        obs = torch.FloatTensor(obs).unsqueeze(0).to(self.device)
        
        episode_reward = 0
        episode_length = 0
        done = False
        
        while not done:
            # èŽ·å–åŠ¨ä½œ
            with torch.no_grad():
                action_probs = self.model(obs)
                dist = Categorical(action_probs)
                action = dist.sample()
                log_prob = dist.log_prob(action)
                value = self.model.get_value(obs)
            
            # æ‰§è¡ŒåŠ¨ä½œ
            step_result = self.env.step(action.item())
            if len(step_result) == 4:
                next_obs, reward, done, info = step_result
            elif len(step_result) == 5:
                next_obs, reward, terminated, truncated, info = step_result
                done = terminated or truncated
            else:
                raise ValueError(f"æœªçŸ¥çš„çŽ¯å¢ƒæ­¥è¿›è¿”å›žå€¼æ ¼å¼: {len(step_result)}")
                
            # æ›´æ–°çŠ¶æ€
            episode_reward += reward
            episode_length += 1
            obs = torch.FloatTensor(next_obs).unsqueeze(0).to(self.device)
            
            # æ£€æŸ¥æ˜¯å¦èŽ·å¾—æ——å¸œ
            if info.get('flag_get', False):
                self.log(f"ðŸŽ‰ Episode {self.episode} èŽ·å¾—æ——å¸œï¼")
                
        return episode_reward, episode_length
        
    def train_step(self):
        """æ‰§è¡Œä¸€æ­¥è®­ç»ƒï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰"""
        # è¿™é‡Œç®€åŒ–è®­ç»ƒé€»è¾‘ï¼Œå®žé™…åº”è¯¥æ”¶é›†æ›´å¤šæ•°æ®
        obs = self.env.reset()
        if isinstance(obs, tuple):
            obs = obs[0]
        obs = torch.FloatTensor(obs).unsqueeze(0).to(self.device)
        
        # å‰å‘ä¼ æ’­
        action_probs = self.model(obs)
        dist = Categorical(action_probs)
        values = self.model.get_value(obs)
        
        # è®¡ç®—æŸå¤±ï¼ˆç®€åŒ–ï¼‰
        policy_loss = -dist.entropy().mean()
        value_loss = nn.MSELoss()(values, torch.zeros_like(values))
        total_loss = policy_loss + self.value_loss_coef * value_loss
        
        # åå‘ä¼ æ’­
        self.optimizer.zero_grad()
        total_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.max_grad_norm)
        self.optimizer.step()
        
        # è®°å½•æŸå¤±
        self.value_losses.append(value_loss.item())
        self.policy_losses.append(policy_loss.item())
        self.total_losses.append(total_loss.item())
        
    def save_model(self, filename):
        """ä¿å­˜æ¨¡åž‹"""
        torch.save(self.model.state_dict(), filename)
        self.log(f"æ¨¡åž‹å·²ä¿å­˜: {filename}")
        
    def train(self, max_episodes=1000, save_interval=100):
        """ä¸»è®­ç»ƒå¾ªçŽ¯"""
        self.log(f"å¼€å§‹è®­ç»ƒï¼Œæœ€å¤§episodes: {max_episodes}")
        self.log(f"å­¦ä¹ çŽ‡: {self.learning_rate}, Gamma: {self.gamma}")
        
        start_time = time.time()
        
        for episode in range(max_episodes):
            self.episode = episode
            
            # æ”¶é›†episodeæ•°æ®
            episode_reward, episode_length = self.collect_episode()
            
            # è®°å½•æŒ‡æ ‡
            self.episode_rewards.append(episode_reward)
            self.total_steps += episode_length
            
            # è¾“å‡ºepisodeä¿¡æ¯
            avg_reward = np.mean(self.episode_rewards[-100:]) if len(self.episode_rewards) >= 100 else np.mean(self.episode_rewards)
            self.log(f"Episode {episode}: å¥–åŠ±={episode_reward:.2f}, é•¿åº¦={episode_length}, å¹³å‡å¥–åŠ±={avg_reward:.2f}")
            
            # å‘é€æŒ‡æ ‡åˆ°å‰ç«¯
            metrics = {
                'episode': episode,
                'reward': episode_reward,
                'length': episode_length,
                'avg_reward': avg_reward,
                'total_steps': self.total_steps,
                'value_loss': np.mean(self.value_losses[-10:]) if self.value_losses else 0,
                'policy_loss': np.mean(self.policy_losses[-10:]) if self.policy_losses else 0,
                'total_loss': np.mean(self.total_losses[-10:]) if self.total_losses else 0
            }
            self.log_metrics(metrics)
            
            # ä¿å­˜æœ€ä½³æ¨¡åž‹
            if episode_reward > self.best_reward:
                self.best_reward = episode_reward
                self.save_model(f"best_model_episode_{episode}.pth")
                
            # å®šæœŸä¿å­˜
            if (episode + 1) % save_interval == 0:
                self.save_model(f"checkpoint_episode_{episode}.pth")
                
            # æ£€æŸ¥æ˜¯å¦åº”è¯¥åœæ­¢è®­ç»ƒ
            if episode >= 100 and avg_reward < -100:
                self.log("è®­ç»ƒæ•ˆæžœä¸ä½³ï¼Œæå‰åœæ­¢")
                break
                
        # è®­ç»ƒå®Œæˆ
        training_time = time.time() - start_time
        self.log(f"è®­ç»ƒå®Œæˆï¼æ€»æ—¶é—´: {training_time/3600:.2f}å°æ—¶")
        self.log(f"æœ€ä½³å¥–åŠ±: {self.best_reward:.2f}")
        self.log(f"æœ€ç»ˆå¹³å‡å¥–åŠ±: {np.mean(self.episode_rewards[-100:]):.2f}")
        
        # ä¿å­˜æœ€ç»ˆæ¨¡åž‹
        self.save_model("final_model.pth")
        
        return {
            'episode_rewards': self.episode_rewards,
            'value_losses': self.value_losses,
            'policy_losses': self.policy_losses,
            'total_losses': self.total_losses,
            'best_reward': self.best_reward,
            'total_steps': self.total_steps
        }

def main():
    parser = argparse.ArgumentParser(description='PPOè®­ç»ƒè„šæœ¬')
    parser.add_argument('--env', default='SuperMarioBros-1-1-v0', help='çŽ¯å¢ƒåç§°')
    parser.add_argument('--action-space', default='SIMPLE', choices=['SIMPLE', 'COMPLEX'], help='åŠ¨ä½œç©ºé—´')
    parser.add_argument('--episodes', type=int, default=1000, help='æœ€å¤§episodes')
    parser.add_argument('--lr', type=float, default=0.0003, help='å­¦ä¹ çŽ‡')
    parser.add_argument('--gamma', type=float, default=0.99, help='æŠ˜æ‰£å› å­')
    parser.add_argument('--config', help='é…ç½®æ–‡ä»¶è·¯å¾„')
    
    args = parser.parse_args()
    
    # åŠ è½½é…ç½®
    if args.config and os.path.exists(args.config):
        with open(args.config, 'r') as f:
            config = json.load(f)
    else:
        config = {
            'env_name': args.env,
            'action_space': args.action_space,
            'learning_rate': args.lr,
            'gamma': args.gamma,
            'clip_ratio': 0.2,
            'value_loss_coef': 0.5,
            'entropy_coef': 0.01,
            'max_grad_norm': 0.5,
            'ppo_epochs': 4
        }
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = PPOTrainer(config)
    
    # å¼€å§‹è®­ç»ƒ
    try:
        results = trainer.train(max_episodes=args.episodes)
        
        # è¾“å‡ºæœ€ç»ˆç»“æžœ
        print(json.dumps({
            'type': 'training_complete',
            'results': results
        }), flush=True)
        
    except KeyboardInterrupt:
        trainer.log("è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        trainer.log(f"è®­ç»ƒå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

if __name__ == '__main__':
    main()
